# Lecture 3

**TLDR;** Lecture 3 primarily focus on the concept of mean differences in a continuous outcome variable (Y, such as BMI) among multiple categorical groups (X, such as weight change categories) using linear regression models. This approach aligns closely with the principles of Analysis of Variance (ANOVA), particularly when dealing with categorical independent variables.

**Simple Linear Regression: Differences in Group Means**

1.  Define the setup of indicator variables for a categorical independent variable

2.  Interpret the regression coefficients for indicator variables in a simple linear regression model

3.  Make inferences regarding the relationships between a continuous response and categorical covariate in a simple linear regression

4.  Use an adjustment procedure such as the Bonferroni correction to make inferences for multiple pairwise comparisons of groups

## Inferences for Differences in Multiple Group Means: Motivating Example and Estimation and Hypothesis Testing

**Indicator Variables for Categorical Independent Variables**:

-   **Explanation**: When you have a categorical independent variable (like type of fruit: apple, banana, orange), you turn these categories into numbers to use them in calculations. For three types of fruits, you use two numbers (like 0 or 1) to represent these categories. These are called indicator variables. $p$ here represents the total number of categories in a categorical variable. In this case, $p$ would be 3 because there are three categories.

-   Instead of using the categories directly in a statistical model, we convert them into a format that the model can use. This is where indicator variables come in. An indicator variable is a binary variable, meaning it can take only two values: 0 or 1.

-   For $p$ categories, you create $p-1$ binary indicator variables. This means for every category except one (the reference category), we have an indicator variable. The reference category is represented by all indicator variables being 0. Example: With 3 weight change categories, you create 2 indicator variables ($X_1$, $X_2$).

**Example**: Imagine you are studying people's preference for fruits: apple, banana, and orange. We use two numbers to represent these:

-   If someone prefers apples, we write 0 for both numbers (meaning neither banana nor orange).

-   If they prefer bananas, we write 1 for the first number and 0 for the second (meaning banana, not orange).

-   If they prefer oranges, we write 0 for the first number and 1 for the second (meaning orange, not banana).

**Interpreting Regression Coefficients for Indicator Variables**:

-   **Explanation**: If we want to predict weight based on diet type, the coefficients (numbers in front of the diet type) tell us how much the diet type affects weight. One number gives the average weight for a standard diet, and other numbers show how much more or less you might weigh if you follow other diets.

-   The regression model is expressed as $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \varepsilon$ , where Y is the outcome (e.g., BMI), $X_1$ and $X_2$ are indicator variables, and $\varepsilon$ is the error term.

    -   $\beta_0$ is the mean value of Y for the reference category (when all Xs are 0).

    -   $\beta_1$ and $\beta_2$ are the differences in the mean values of Y from the reference category.

    -   $\beta_1$ = $\mu_1$ − $\mu_0$

    -   $\hat{\beta_0}=$ average BMI in maintain group

    -   $\hat{\beta_0} + \hat{\beta_1} =$ average BMI in lose group

    -   $\hat{\beta_0} + \hat{\beta_2} =$ average BMI in gain group

    -   $\hat{\beta_1} =$ difference in average BMI between lose and maintain groups

    -   $\hat{\beta_2} =$ difference in average BMI between gain and maintain groups

-   Possible hypotheses could be

    -   $H_0$: $\mu_1$ = $\mu_2$ versus $H_\alpha$: $\mu_1$ ≠ $\mu_2$ , where $\mu_1$ = mean BMI in the group seeking to **maintain** weight

    -   $H_0$: $\mu_1$ = $\mu_3$ versus $H_\alpha$: $\mu_1$ ≠ $\mu_3$ , where $\mu_2$ = mean BMI in the group seeking to **lose** weight

    -   $H_0$: $\mu_2$ = $\mu_3$ versus $H_\alpha$: $\mu_2$ ≠ $\mu_3$ , where $\mu_3$ = mean BMI in the group seeking to **gain** weight

**Example**: Suppose we're studying how vegetarian and meat diets affect weight. The formula might look like this: $Weight = 60 + 5 Vegetarian + 10 Meat$. Here, 60 is the average weight for someone not on a specific diet. The 5 tell us that vegetarians weigh on average 5 units more and the 10 means meat-eaters 10 units more than someone not following any specific diet.

## Inference for Differences in Multiple Group Means: BMI Example and Multiple Comparisons

**Making Inferences about Relationships**:

-   **Explanation**: This is like checking if different diets really affect weight. We do some math (t-tests and F-tests) to see if the differences we see (like vegetarians weighing more) are true or just by chance.

    -   We define $\hat{\beta_1}$ = $b_1$ = estimated difference in means for groups 1 and 0

    -   Test $H_0$: $\beta_1$ = 0 (that is there is no mean difference between the groups) using a t-test of $t = \frac{\hat{\beta}_1 - 0}{SE(\hat{\beta}_1)}$

    -   Construct $100 (1-\alpha) \%$ CI for $\beta_1$ using $\hat{\beta}_1 \pm t_{\alpha/2, n-2} \cdot SE(\hat{\beta}_1)$

-   The hypothesis testing checks if there are significant differences in group means. For example, testing if $\beta_1$ and $\beta_2$ are significantly different from 0 using t-tests.

-   An F-test assesses whether at least one of the group means is significantly different from others.

-   **Example**: After collecting weights of people on different diets, we use t-tests to see if vegetarians or meat-eaters really weigh more or if the differences are just random. If our tests show the differences are real, we can say the diet type does affect weight.

**Adjustment Procedures for Multiple Pairwise Comparisons**:

-   **Explanation**: When comparing many things (like different diets), the chances of mistakenly finding a difference increase. To avoid this, we adjust our criteria for finding a real difference. This means we are more strict in deciding if a diet really affects weight.

-   When making multiple comparisons, such as comparing mean differences across several groups, the risk of Type I error (false positives) increases.

-   The **Bonferroni correction** is used to control this risk by dividing the desired overall alpha level by the number of comparisons.

-   For instance, if you have 3 comparisons and an alpha level of 0.05, the Bonferroni-adjusted alpha level would be $\frac{0.05}{3} = 0.0167$. With the Bonferroni correction, you would only consider a result to be statistically significant if the p-value (the probability of observing the result if the null hypothesis is true) is below 0.0167, instead of the usual 0.05.

-   This means you are being more cautious and reducing the risk of falsely detecting a significant effect. In other words, a result must show stronger evidence to be considered significant when making multiple comparisons. ***Think of it like setting a higher standard for evidence when you have more tests to conduct.***

-   **Example**: Imagine testing if vegetarian, vegan, and meat diets affect weight. Normally, we might say there's a real difference if there's less than a 5% chance it's random (alpha level of 0.05). But with three diets, we adjust this to be stricter, like less than a 2% chance (0.05 divided by the number of comparisons, which is 3 in this case).

## All Formulas & Graphs

-   **Linear Regression Model for Categorical Independent Variables:**

$Y_{ij} = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$

Where $Y_{ij}$ is the response variable for the $i^{th}$ individual in the $j^{th}$group, $\beta_0, \beta_1, \beta_2$ are regression coefficients, $X_1, X_2$ are indicator variables for the categorical independent variable, and $\epsilon$is the error term.

-   **Indicator Variable Definitions:**

$X_1 = \begin{cases} 1 & \text{if group 1} \\ 0 & \text{otherwise} \end{cases}$

$X_2 = \begin{cases} 1 & \text{if group 2} \\ 0 & \text{otherwise} \end{cases}$

These define the indicator variables for the groups in the categorical variable.

-   **Interpretation of Regression Coefficients:**

\- For group 1: $\hat{Y} = \beta_0 + \beta_1$

\- For group 2: $\hat{Y} = \beta_0 + \beta_2$

\- For the reference group: $\hat{Y} = \beta_0$

-   **Hypothesis Testing for Regression Coefficients:**

\- Null Hypothesis: $H_0: \beta_1 = \beta_2 = 0$

\- Alternative Hypothesis: $H_1:$ at least one $\beta$ is not equal to 0.

-   **F-test for Regression Model:**

$F = \frac{\text{Mean Square for Regression}}{\text{Mean Square for Error}}$

-   **T-test for Individual Coefficients:**

$t = \frac{\text{Coefficient} - \text{Hypothesized Value}}{\text{Standard Error of Coefficient}}$

-   **Confidence Interval for Coefficients:**

$\text{CI for } \beta_j = \text{Estimated Coefficient} \pm (\text{t-value} \times \text{Standard Error of Coefficient})$

-   **General Linear Regression Model for p Groups:**

$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_{p-1} X_{p-1} + \epsilon$

-   **Bonferroni Correction for Multiple Comparisons:**

Adjusted Significance Level: $\alpha^* = \frac{\alpha}{\text{number of comparisons}}$

### **Relationships Among Formulas**

-   The indicator variables ($X_1$, $X_2$) in the regression equation are used to represent the categories of the categorical independent variable.

-   $\beta_0$, $\beta_1$, and $\beta_2$ in the regression equation represent the mean of the reference group and the mean differences from the reference group, respectively.

-   The hypothesis tests (t-tests for individual $\beta$ coefficients and F-test for the overall model) are used to infer if these mean differences are statistically significant.

-   When multiple comparisons are involved, the Bonferroni correction adjusts the significance level to account for the increased risk of Type I error.

### STATA Output

![STATA Output for BMI Example](images/STATA%20Output.png){alt="STATA Output for BMI Example"}

| **Section**  | **Item**         | **Value**  | **Interpretation**                                                                                                                                                    |
|-----------|-----------|-----------|---------------------------------------|
| **Model**    | Source SS        | 1443.83    | Sum of Squares due to the model, measuring how much of the total variability in BMI is explained by the weight change categories.                                     |
|              | df               | 2          | Degrees of freedom for the model, equal to the number of categories minus 1 (3 categories - 1).                                                                       |
|              | MS               | 721.91     | Mean Square for the model, calculated as Source SS divided by df. Indicates the average squared deviation of the model predictions from the mean BMI.                 |
| **Residual** | Source SS        | 6136.83    | Sum of Squares due to residuals, representing the variability in BMI not explained by the model.                                                                      |
|              | df               | 425        | Degrees of freedom for the residuals, calculated as total number of observations minus the number of parameters estimated (428 - 3).                                  |
|              | MS               | 14.44      | Mean Square for residuals, computed as Residual Source SS divided by its df. Indicates the average squared deviation of the observed BMIs from the model predictions. |
| **Total**    | Source SS        | 7580.66    | Total variability in BMI across all observations.                                                                                                                     |
|              | df               | 427        | Total degrees of freedom, equal to the number of observations minus 1.                                                                                                |
|              | MS               | 17.75      | Total Mean Square, representing the average squared deviation of the BMI values from their overall mean.                                                              |
| **Overall**  | Number of obs    | 428        | Total number of observations in the study.                                                                                                                            |
|              | F                | 50.00      | F-statistic, calculated as Model MS divided by Residual MS. Measures the overall significance of the model.                                                           |
|              | Prob \> F        | 0.00       | Probability of observing a more extreme F-statistic under the null hypothesis. A value close to 0 suggests the model is significant.                                  |
|              | R-squared        | 0.19       | Proportion of the total variation in BMI explained by the model.                                                                                                      |
|              | Adj R-squared    | 0.19       | Adjusted R-squared, accounting for the number of predictors in the model. More reliable for comparing models with different numbers of predictors.                    |
|              | Root MSE         | 3.80       | Root Mean Squared Error, indicating the standard deviation of the residuals.                                                                                          |
| **BMI**      | Coef. (Lose)     | 3.58       | The estimated increase in BMI for those who wish to lose weight, compared to the reference category (maintain weight).                                                |
|              | Std. Err. (Lose) | .38        | Standard error of the coefficient for losing weight, indicating the precision of the estimate.                                                                        |
|              | t (Lose)         | 9.40       | t-statistic for the lose weight category, used for hypothesis testing.                                                                                                |
|              | P\>              | t          | (Lose)                                                                                                                                                                |
|              | CI (Lose)        | 2.83, 4.33 | 95% confidence interval for the coefficient for losing weight. Indicates the range within which the true coefficient is likely to fall.                               |
|              | Coef. (Gain)     | −.579      | The estimated change in BMI for those who wish to gain weight, compared to the reference category. Negative value indicates a decrease.                               |
|              | Std. Err. (Gain) | .75        | Standard error of the coefficient for gaining weight.                                                                                                                 |
|              | t (Gain)         | −0.77      | t-statistic for the gain weight category.                                                                                                                             |
|              | P\>              | t          | (Gain)                                                                                                                                                                |
|              | CI (Gain)        | −2.05, .89 | 95% confidence interval for the coefficient for gaining weight.                                                                                                       |
|              | Coef. (\_cons)   | 21.49      | The constant                                                                                                                                                          |
